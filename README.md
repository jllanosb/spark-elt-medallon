# ğŸ† Spark ETL - Arquitectura MedallÃ³n

Proyecto educativo de ingenierÃ­a de datos que implementa un pipeline ETL con Apache Spark siguiendo el patrÃ³n de arquitectura MedallÃ³n (Bronze â†’ Silver â†’ Gold), adaptado a capas: Workload â†’ Landing â†’ Curated â†’ Functional.

# ğŸ“‹ Tabla de Contenidos
- ğŸ¯ Â¿QuÃ© es este proyecto?
- ğŸ—ï¸ Arquitectura MedallÃ³n Explicada
- ğŸ“ Estructura del Repositorio
- âš™ï¸ TecnologÃ­as Utilizadas
- ğŸš€ GuÃ­a de EjecuciÃ³n Paso a Paso
- ğŸ” Detalle de Cada Capa
- ğŸ“Š Esquema de Datos
- ğŸ’¡ Mejores PrÃ¡cticas Implementadas
- ğŸ”§ SoluciÃ³n de Problemas Comunes
- ğŸ“š Recursos de Aprendizaje

# ğŸ¯ Â¿QuÃ© es este proyecto?
Este repositorio es una implementaciÃ³n didÃ¡ctica de un pipeline de datos empresarial usando Apache Spark y Hadoop Ecosystem. Su objetivo principal es:
- âœ… EnseÃ±ar los fundamentos de la arquitectura MedallÃ³n en entornos on-premise
- âœ… Demonstrar buenas prÃ¡cticas de ingesta, transformaciÃ³n y calidad de datos
- âœ… Proveer cÃ³digo reutilizable para procesos ETL escalables
- âœ… Facilitar el aprendizaje de Spark SQL, Hive y formatos columnares  

ğŸ’¡ Caso de uso: Procesamiento de transacciones comerciales con entidades PERSONA, EMPRESA y TRANSACCION, aplicando reglas de calidad y enriquecimiento progresivo.

# ğŸ—ï¸ Arquitectura MedallÃ³n Explicada
La arquitectura MedallÃ³n organiza los datos en capas de refinamiento progresivo, mejorando la calidad y utilidad en cada etapa:
```table
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           FLUJO DE DATOS                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  ğŸ“¥ FUENTES â†’ ğŸ¥‰ WORKLOAD â†’ ğŸ¥ˆ LANDING â†’ ğŸ¥‡ CURATED â†’ âš¡ FUNCTIONAL    
â”‚              (Bronze)      (Silver)     (Gold)      (Analytics)         â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
ğŸ”¹ Capa 1: WORKLOAD (Bronze - Datos Crudos)
```table
CaracterÃ­stica      DescripciÃ³n
-----------------------------------------------------
Formato             TEXTFILE con delimitador `
Encoding            ISO-8859-1 (soporte legacy)
PropÃ³sito           Ingesta fiel de fuentes originales
ValidaciÃ³n          MÃ­nima (solo estructura)
```
ğŸ”¹ Capa 2: LANDING (Silver - Datos Estandarizados)
```table
CaracterÃ­stica      DescripciÃ³n
------------------------------------------------------
Formato             AVRO con compresiÃ³n Snappy 
Schema              Definido en archivos .avsc
PropÃ³sito           Estructura consistente + metadatos
Particionamiento    Por fecha en tablas transaccionales
```
ğŸ”¹ Capa 3: CURATED (Gold - Datos Limpios)
```table
CaracterÃ­stica      DescripciÃ³n
--------------------------------------------------
Formato             Parquet con Snappy
Calidad             Reglas de validaciÃ³n aplicadas
Tipado              ConversiÃ³n explÃ­cita de tipos
PropÃ³sito           Datos confiables para anÃ¡lisis
```
ğŸ”¹ Capa 4: FUNCTIONAL (Analytics - Datos Enriquecidos)
```table
CaracterÃ­stica      DescripciÃ³n
--------------------------------------------------------
Formato             Parquet optimizado
TransformaciÃ³n      JOINs para enriquecimiento semÃ¡ntico
OptimizaciÃ³n        Broadcast joins para tablas pequeÃ±as
PropÃ³sito           Listo para dashboards y ML
```
ğŸ“š La arquitectura MedalliÃ³n es ampliamente adoptada en plataformas como Databricks y Azure Synapse para organizar data lakes de forma escalable.

# ğŸ“ Estructura del Repositorio
```text
spark-elt-medallon/
â”‚
â”œâ”€â”€ ğŸ“ dataset/                   # Datos fuente de ejemplo
â”‚   â”œâ”€â”€ empresa.data              # CatÃ¡logo de empresas (pipe-delimited)
â”‚   â”œâ”€â”€ persona.data              # Registro de personas
â”‚   â””â”€â”€ transacciones.data        # Movimientos comerciales
â”‚
â”œâ”€â”€ ğŸ“ schema/                    # Esquemas Avro para validaciÃ³n
â”‚   â”œâ”€â”€ empresa.avsc              # Schema: id, nombre
â”‚   â”œâ”€â”€ persona.avsc              # Schema: id, nombre, contacto, etc.
â”‚   â””â”€â”€ transaccion.avsc          # Schema: monto, fecha, relaciones
â”‚
â”œâ”€â”€ ğŸ“ procesos/                  # Scripts PySpark del pipeline
â”‚   â”œâ”€â”€ poblar_capa_workload.py   # â–¶ï¸ Ingesta inicial (CSV â†’ Hive TEXTFILE)
â”‚   â”œâ”€â”€ poblar_capa_landing.py    # â–¶ï¸ EstandarizaciÃ³n (â†’ Avro + particiÃ³n)
â”‚   â”œâ”€â”€ poblar_capa_curated.py    # â–¶ï¸ Limpieza y validaciÃ³n de calidad
â”‚   â””â”€â”€ poblar_capa_functional.py # â–¶ï¸ Enriquecimiento con JOINs
â”‚
â”œâ”€â”€ ğŸ“„ instrucciones.txt          # GuÃ­a rÃ¡pida de comandos de ejecuciÃ³n
â””â”€â”€ ğŸ“„ README.md                  # Â¡Este archivo! DocumentaciÃ³n didÃ¡ctica
```
# âš™ï¸ TecnologÃ­as Utilizadas
```table
TecnologÃ­a          VersiÃ³n         PropÃ³sito
------------------------------------------------------------------------
Apache Spark        3.5.0           Motor de procesamiento distribuido
Apache Hive         3.x             Metastore y consulta SQL sobre HDFS
Hadoop HDFS         3.x             Almacenamiento distribuido
Apache YARN         3.x             Gestor de recursos del cluster
Formato Avro        1.11+           SerializaciÃ³n con esquema evolutivo
Formato Parquet     1.12+           Almacenamiento columnar optimizado
CompresiÃ³n Snappy   1.1+            Balance velocidad/tamaÃ±o en datos
```
ğŸ”— Estas herramientas son estÃ¡ndar en ecosistemas de Big Data on-premise y en la nube

# ğŸš€ GuÃ­a de EjecuciÃ³n Paso a Paso

ğŸ”¹ Prerrequisitos
```text
# Cluster Hadoop con servicios activos:
âœ… HDFS en ejecuciÃ³n
âœ… YARN Resource Manager
âœ… Hive Metastore + HiveServer2
âœ… Spark instalado y configurado con Hive
âœ… Acceso SSH al nodo edge con usuario `hadoop`
```
ğŸ”¹ Paso 1: Iniciar servicios (si es necesario)
# Desde instrucciones.txt
```bash
start-dfs.sh
start-yarn.sh
hive --service metastore &
sleep 10
hive --service hiveserver2 &
```
ğŸ”¹ Paso 2: Cargar datos fuente a HDFS
# Crear directorio y subir archivos .data
```bash
hdfs dfs -mkdir -p /user/hadoop/dataset
hdfs dfs -put /home/hadoop/spark-elt-medallon/dataset/* /user/hadoop/dataset/
hdfs dfs -ls /user/hadoop/dataset  # Verificar carga
```
ğŸ”¹ Paso 3: Ejecutar cada capa del pipeline
ğŸ¥‰ Capa WORKLOAD (Ingesta)
```pyspark
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --conf spark.sql.warehouse.dir=/user/hadoop/warehouse \
  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
  /home/hadoop/spark-elt-medallon/procesos/poblar_capa_workload.py \
  --env TopicosB \
  --username hadoop \
  --base_path /user \
  --local_data_path /user/hadoop/dataset
```
ğŸ¥ˆ Capa LANDING (EstandarizaciÃ³n Avro)

- Primero subir esquemas Avro
```bash
hdfs dfs -mkdir -p /user/hadoop/datalake/schema/TOPICOSB_LANDING/
hdfs dfs -put -f /home/hadoop/spark-elt-medallon/schema/*.avsc /user/hadoop/datalake/schema/TOPICOSB_LANDING/
hdfs dfs -ls /user/hadoop/datalake/schema/TOPICOSB_LANDING/
```
- Ejecutar proceso
```pyspark
spark-submit \
  --master yarn \
  --deploy-mode client \
  --conf spark.sql.warehouse.dir=/user/hadoop/warehouse \
  --conf spark.sql.avro.compression.codec=snappy \
  --packages org.apache.spark:spark-avro_2.12:3.5.0 \
  /home/hadoop/spark-elt-medallon/procesos/poblar_capa_landing.py \
  --env TopicosB \
  --username hadoop \
  --base_path /user \
  --schema_path /user/hadoop/datalake/schema \
  --source_db topicosb_workload
```
ğŸ¥‡ Capa CURATED (Calidad y Limpieza)
```pyspark
spark-submit \
  --master yarn \
  --deploy-mode client \
  --conf spark.sql.warehouse.dir=/user/hadoop/warehouse \
  --conf spark.sql.parquet.compression.codec=snappy \
  --conf spark.dynamicAllocation.enabled=true \
  --conf spark.executor.instances=10 \
  --conf spark.executor.memory=4g \
  --conf spark.driver.memory=2g \
  /home/hadoop/spark-elt-medallon/procesos/poblar_capa_curated.py \
  --env TopicosB \
  --username hadoop \
  --base_path /user \
  --source_db landing \
  --enable-validation  # â† Activa filtros de calidad
```
âš¡ Capa FUNCTIONAL (Enriquecimiento)
```pyspark
spark-submit \
  --master yarn \
  --deploy-mode client \
  --conf spark.sql.warehouse.dir=/user/hadoop/warehouse \
  --conf spark.yarn.queue=default \
  --conf spark.sql.parquet.compression.codec=snappy \
  --conf spark.dynamicAllocation.enabled=false \
  /home/hadoop/spark-elt-medallon/procesos/poblar_capa_functional.py \
  --env TopicosB \
  --username hadoop \
  --base_path /user \
  --source_db curated \
  --num-executors 8 \
  --executor-memory 2g \
  --executor-cores 2 \
  --enable-broadcast  # â† Optimiza JOINs con tablas pequeÃ±as
```
ğŸ”¹ Paso 4: Detener servicios (opcional)
```bash
stop-yarn.sh
stop-dfs.sh
pkill -f HiveServer2
pkill -f HiveMetaStore
```
ğŸ“Š Esquema de Datos
Entidad: PERSONA
```table
Campo       Tipo Original       Tipo Final      Regla de Calidad
---------------------------------------------------------------------------
ID          String              String          NOT NULL
NOMBRE      String              String          -
EDAD        String              Integer         BETWEEN 1 AND 99
SALARIO     String              Double          BETWEEN 0.01 AND 9999999.99
ID_EMPRESA  String              String          NOT NULL
```
Entidad: TRANSACCION_ENRIQUECIDA (Functional)
```table
Campo               Origen              TransformaciÃ³n
--------------------------------------------------------------------
ID_PERSONA          TRANSACCION         Clave de join
NOMBRE_PERSONA      PERSONA.NOMBRE      Enriquecimiento semÃ¡ntico
EDAD_PERSONA        PERSONA.EDAD        ConversiÃ³n + validaciÃ³n
TRABAJO_PERSONA     EMPRESA.NOMBRE      JOIN con empresa empleadora
MONTO_TRANSACCION   TRANSACCION.MONTO   ConversiÃ³n a Double
EMPRESA_TRANSACCION EMPRESA.NOMBRE      JOIN con empresa receptora
FECHA_TRANSACCION   TRANSACCION.FECHA   Columna de particiÃ³n
```
# ğŸ’¡ Mejores PrÃ¡cticas Implementadas
- âœ… Esquemas explÃ­citos: Evita inferencia automÃ¡tica y garantiza consistencia
- âœ… ValidaciÃ³n progresiva: Reglas de calidad aplicadas en capa Curated
- âœ… Particionamiento inteligente: Por fecha en tablas transaccionales para consultas eficientes
- âœ… CompresiÃ³n Snappy: Balance Ã³ptimo entre velocidad y almacenamiento 
- âœ… Broadcast joins: OptimizaciÃ³n automÃ¡tica para tablas de dimensiÃ³n pequeÃ±as
- âœ… Logging estructurado: Mensajes claros para monitoreo y debugging
- âœ… ParÃ¡metros configurables: --env, --enable-validation, --enable-broadcast para flexibilidad
- âœ… Limpieza de recursos: spark.stop() y eliminaciÃ³n de vistas temporales  

 Estas prÃ¡cticas siguen recomendaciones de Databricks y Microsoft para pipelines productivos

# ğŸ” Comandos de diagnÃ³stico Ãºtiles
```bash
# Verificar archivos en HDFS
hdfs dfs -ls /user/hadoop/datalake/TOPICOSB_LANDING/

# Consultar metadatos de tabla Hive
hive -e "DESCRIBE FORMATTED topicosb_landing.persona;"
```
```sql
# Contar registros por particiÃ³n
spark.sql("SELECT FECHA_TRANSACCION, COUNT(*) FROM topicosb_functional.transaccion_enriquecida GROUP BY FECHA_TRANSACCION").show()
```
```bash
# Monitorear aplicaciÃ³n Spark en YARN
yarn application -list | grep "Proceso_Carga"
```
# ğŸ¤ Contribuciones
Este proyecto estÃ¡ diseÃ±ado para fines educativos. Â¡Las contribuciones son bienvenidas!

âœ… Ideas para mejorar:
- [ ] Agregar tests unitarios con pytest y chispa
- [ ] Implementar lineage de datos con OpenLineage
- [ ] AÃ±adir dashboard de monitoreo con Prometheus/Grafana
- [ ] Soporte para Delta Lake como formato unificado
- [ ] Docker-compose para entorno de desarrollo local

ğŸ·ï¸ Licencia: MIT - Libre uso para fines educativos y de investigaciÃ³n

ğŸ‘¨â€ğŸ’» Autor: @jllanosb

ğŸ“… Ãšltima actualizaciÃ³n: Febrero 2026

Contexto: Desarrollado con enfoque en formaciÃ³n en ingenierÃ­a de datos en entornos on-premise

# âœ¨ "La calidad de los datos no es un paso, es un viaje a travÃ©s de capas de refinamiento" âœ¨