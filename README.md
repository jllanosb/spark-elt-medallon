# ğŸ† Spark ETL - Arquitectura MedallÃ³n

Proyecto educativo de ingenierÃ­a de datos que implementa un pipeline ETL con Apache Spark siguiendo el patrÃ³n de arquitectura MedallÃ³n (Bronze â†’ Silver â†’ Gold), adaptado a capas: Workload â†’ Landing â†’ Curated â†’ Functional, con integraciÃ³n completa: Hive â†’ Parquet â†’ CSV â†’ MongoDB.

# ğŸ“‹ Tabla de Contenidos
- ğŸ¯ Â¿QuÃ© es este proyecto?
- ğŸ”„ Novedades: ExportaciÃ³n CSV + MongoDB
- ğŸ—ï¸ Arquitectura MedallÃ³n Explicada
- ğŸ“ Estructura del Repositorio
- âš™ï¸ TecnologÃ­as Utilizadas
- ğŸš€ GuÃ­a de EjecuciÃ³n Paso a Paso (9 Pasos)
- ğŸ” Detalle de Cada Capa
- ğŸ“Š Esquema de Datos
- ğŸ“¤ ExportaciÃ³n: Hive â†’ CSV â†’ MongoDB
- ğŸ’¡ Mejores PrÃ¡cticas Implementadas
- ğŸ”§ SoluciÃ³n de Problemas Comunes
- ğŸ“š Recursos de Aprendizaje

# ğŸ¯ Â¿QuÃ© es este proyecto?
Este repositorio es una implementaciÃ³n didÃ¡ctica de un pipeline de datos empresarial usando Apache Spark y Hadoop Ecosystem. Su objetivo principal es:
- âœ… EnseÃ±ar los fundamentos de la arquitectura MedallÃ³n en entornos on-premise
- âœ… Demostrar buenas prÃ¡cticas de ingesta, transformaciÃ³n y calidad de datos
- âœ… Proveer cÃ³digo reutilizable para procesos ETL escalables
- âœ… Facilitar el aprendizaje de Spark SQL, Hive y formatos columnares
- âœ… Integrar mÃºltiples destinos: Hive, archivos CSV y MongoDB para diferentes casos de uso    

ğŸ’¡ Caso de uso completo: Procesamiento de transacciones comerciales con entidades PERSONA, EMPRESA y TRANSACCION, aplicando reglas de calidad, enriquecimiento progresivo y exportaciÃ³n a sistemas operacionales (MongoDB) para aplicaciones en tiempo real.

# ğŸ—ï¸ Arquitectura MedallÃ³n Explicada
La arquitectura MedallÃ³n organiza los datos en capas de refinamiento progresivo, mejorando la calidad y utilidad en cada etapa:
```table
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                         FLUJO DE DATOS                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ 
â”‚                                                                         
â”‚ğŸ“¥ Fuentes â†’ ğŸ¥‰ Workload â†’ ğŸ¥ˆ Landing â†’ ğŸ¥‡ Curated â†’ âš¡ Functional â†’ ğŸ“„ gold.csv â†’ ğŸ—„ï¸ MongoDB
â”‚              (HDFS)         (Avro)        (Parquet)    (Parquet)     (Export)    (NoSQL)
â”‚           (Bronze)      (Silver)     (Gold)      (Analytics)         
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```
ğŸ”¹ Capa 1: WORKLOAD (Bronze - Datos Crudos)
```table
CaracterÃ­stica      DescripciÃ³n
-----------------------------------------------------
Formato             TEXTFILE con delimitador `
Encoding            ISO-8859-1 (soporte legacy)
PropÃ³sito           Ingesta fiel de fuentes originales
ValidaciÃ³n          MÃ­nima (solo estructura)
```
ğŸ”¹ Capa 2: LANDING (Silver - Datos Estandarizados)
```table
CaracterÃ­stica      DescripciÃ³n
------------------------------------------------------
Formato             AVRO con compresiÃ³n Snappy 
Schema              Definido en archivos .avsc
PropÃ³sito           Estructura consistente + metadatos
Particionamiento    Por fecha en tablas transaccionales
```
ğŸ”¹ Capa 3: CURATED (Gold - Datos Limpios)
```table
CaracterÃ­stica      DescripciÃ³n
--------------------------------------------------
Formato             Parquet con Snappy
Calidad             Reglas de validaciÃ³n aplicadas
Tipado              ConversiÃ³n explÃ­cita de tipos
PropÃ³sito           Datos confiables para anÃ¡lisis
```
ğŸ”¹ Capa 4: FUNCTIONAL (Analytics - Datos Enriquecidos)
```table
CaracterÃ­stica      DescripciÃ³n
--------------------------------------------------------
Formato             Parquet optimizado
TransformaciÃ³n      JOINs para enriquecimiento semÃ¡ntico
OptimizaciÃ³n        Broadcast joins para tablas pequeÃ±as
PropÃ³sito           Listo para dashboards y ML
```
ğŸ“š La arquitectura MedalliÃ³n es ampliamente adoptada en plataformas como Databricks y Azure Synapse para organizar data lakes de forma escalable.

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FLUJO DE DATOS COMPLETO                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                                                    
â”‚  ğŸ“¥ FUENTES                                                               
â”‚     â†“                                                                     
â”‚  ğŸ¥‰ WORKLOAD (Bronze) â†’ Datos crudos en HDFS (TEXTFILE \| pipe-delimited) 
â”‚     â†“                                                                   
â”‚  ğŸ¥ˆ LANDING (Silver)  â†’ EstandarizaciÃ³n con Avro + particiÃ³n por fecha  
â”‚     â†“                                                                   
â”‚  ğŸ¥‡ CURATED (Gold)    â†’ Limpieza, validaciÃ³n y tipado fuerte (Parquet)  
â”‚     â†“                                                                   
â”‚  âš¡ FUNCTIONAL        â†’ Enriquecimiento con JOINs (Parquet optimizado)  
â”‚     â†“                                                                   
â”‚  ğŸ“„ EXPORT CSV        â†’ ExtracciÃ³n a gold.csv para interoperabilidad  
â”‚     â†“                                                                   
â”‚  ğŸ—„ï¸ MONGODB          â†’ Carga a colecciÃ³n NoSQL para aplicaciones
â”‚                                                                         
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

# ğŸ“ Estructura del Repositorio
```text
spark-elt-medallon/
â”‚
â”œâ”€â”€ ğŸ“ dataset/                    # Datos fuente de ejemplo
â”‚   â”œâ”€â”€ empresa.data              # CatÃ¡logo de empresas (pipe-delimited)
â”‚   â”œâ”€â”€ persona.data              # Registro de personas
â”‚   â””â”€â”€ transacciones.data        # Movimientos comerciales
â”‚
â”œâ”€â”€ ğŸ“ schema/                     # Esquemas Avro para validaciÃ³n
â”‚   â”œâ”€â”€ empresa.avsc
â”‚   â”œâ”€â”€ persona.avsc
â”‚   â””â”€â”€ transaccion.avsc
â”‚
â”œâ”€â”€ ğŸ“ procesos/                   # Scripts PySpark del pipeline
â”‚   â”œâ”€â”€ poblar_capa_workload.py   # â–¶ï¸ Ingesta: CSV â†’ Hive TEXTFILE
â”‚   â”œâ”€â”€ poblar_capa_landing.py    # â–¶ï¸ EstandarizaciÃ³n: â†’ Avro + particiÃ³n
â”‚   â”œâ”€â”€ poblar_capa_curated.py    # â–¶ï¸ Limpieza y validaciÃ³n de calidad
â”‚   â”œâ”€â”€ poblar_capa_functional.py # â–¶ï¸ Enriquecimiento con JOINs
â”‚   â”œâ”€â”€ export_gold_to_csv.py     # ğŸ†• ExtracciÃ³n: Hive Functional â†’ CSV
â”‚   â””â”€â”€ export_gold_to_mongo.py   # ğŸ†• Carga: gold.csv â†’ MongoDB
â”‚
â”œâ”€â”€ ğŸ“ datalake/                   # Rutas HDFS generadas (no versionadas)
â”‚   â”œâ”€â”€ temp/                     # Archivos temporales de exportaciÃ³n
â”‚   â””â”€â”€ gold.csv                  # Archivo final consolidado
â”‚
â”œâ”€â”€ ğŸ“„ instrucciones.txt          # GuÃ­a rÃ¡pida de comandos (9 pasos)
â””â”€â”€ ğŸ“„ README.md                  # Â¡Este archivo! DocumentaciÃ³n didÃ¡ctica
```
# âš™ï¸ TecnologÃ­as Utilizadas
```table
TecnologÃ­a              VersiÃ³n         PropÃ³sito
------------------------------------------------------------------------
Apache Spark            3.5.0           Motor de procesamiento distribuido
Apache Hive             3.x             Metastore y consulta SQL sobre HDFS
Hadoop HDFS             3.x             Almacenamiento distribuido
Apache YARN             3.x             Gestor de recursos del cluster
Formato Avro            1.11+           SerializaciÃ³n con esquema evolutivo
Formato Parquet         1.12+           Almacenamiento columnar optimizado
CompresiÃ³n Snappy       1.1+            Balance velocidad/tamaÃ±o en datos
MongoDB Spark Connector 10.4.0          IntegraciÃ³n bidireccional Spark â†” MongoDB
PyMongo / Spark MongoDB Compatible      Lectura/escritura eficiente a NoSQL
```
ğŸ”— Estas herramientas son estÃ¡ndar en ecosistemas de Big Data on-premise y en la nube

# ğŸš€ GuÃ­a de EjecuciÃ³n Paso a Paso

## ğŸ”¹ Prerrequisitos
```text
# Cluster Hadoop con servicios activos:
âœ… HDFS en ejecuciÃ³n
âœ… YARN Resource Manager
âœ… Hive Metastore + HiveServer2
âœ… Spark instalado y configurado con Hive
âœ… Acceso SSH al nodo edge con usuario `hadoop`
âœ… MongoDB instalado y accesible (local o remoto)
```
## ğŸ”¹ Paso 1: Iniciar servicios (si es necesario)
### Desde instrucciones.txt
```bash
start-dfs.sh
start-yarn.sh
hive --service metastore &
sleep 10
hive --service hiveserver2 &
```
## ğŸ”¹ Paso 2: Cargar datos fuente a HDFS
### Crear directorio y subir archivos .data
```bash
hdfs dfs -mkdir -p /user/hadoop/dataset
hdfs dfs -put /home/hadoop/spark-elt-medallon/dataset/* /user/hadoop/dataset/
hdfs dfs -ls /user/hadoop/dataset  # Verificar carga
```
## ğŸ”¹ Paso 3: Ejecutar cada capa del pipeline
### ğŸ¥‰ Capa WORKLOAD (Ingesta)
```pyspark
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --conf spark.sql.warehouse.dir=/user/hadoop/warehouse \
  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
  /home/hadoop/spark-elt-medallon/procesos/poblar_capa_workload.py \
  --env TopicosB \
  --username hadoop \
  --base_path /user \
  --local_data_path /user/hadoop/dataset
```
### ğŸ¥ˆ Capa LANDING (EstandarizaciÃ³n Avro)

- Primero subir esquemas Avro
```bash
hdfs dfs -mkdir -p /user/hadoop/datalake/schema/TOPICOSB_LANDING/
hdfs dfs -put -f /home/hadoop/spark-elt-medallon/schema/*.avsc /user/hadoop/datalake/schema/TOPICOSB_LANDING/
hdfs dfs -ls /user/hadoop/datalake/schema/TOPICOSB_LANDING/
```
- Ejecutar proceso
```pyspark
spark-submit \
  --master yarn \
  --deploy-mode client \
  --conf spark.sql.warehouse.dir=/user/hadoop/warehouse \
  --conf spark.sql.avro.compression.codec=snappy \
  --packages org.apache.spark:spark-avro_2.12:3.5.0 \
  /home/hadoop/spark-elt-medallon/procesos/poblar_capa_landing.py \
  --env TopicosB \
  --username hadoop \
  --base_path /user \
  --schema_path /user/hadoop/datalake/schema \
  --source_db topicosb_workload
```
### ğŸ¥‡ Capa CURATED (Calidad y Limpieza)
```pyspark
spark-submit \
  --master yarn \
  --deploy-mode client \
  --conf spark.sql.warehouse.dir=/user/hadoop/warehouse \
  --conf spark.sql.parquet.compression.codec=snappy \
  --conf spark.dynamicAllocation.enabled=true \
  --conf spark.executor.instances=10 \
  --conf spark.executor.memory=4g \
  --conf spark.driver.memory=2g \
  /home/hadoop/spark-elt-medallon/procesos/poblar_capa_curated.py \
  --env TopicosB \
  --username hadoop \
  --base_path /user \
  --source_db landing \
  --enable-validation  # â† Activa filtros de calidad
```
### âš¡ Capa FUNCTIONAL (Enriquecimiento)
```pyspark
spark-submit \
  --master yarn \
  --deploy-mode client \
  --conf spark.sql.warehouse.dir=/user/hadoop/warehouse \
  --conf spark.yarn.queue=default \
  --conf spark.sql.parquet.compression.codec=snappy \
  --conf spark.dynamicAllocation.enabled=false \
  /home/hadoop/spark-elt-medallon/procesos/poblar_capa_functional.py \
  --env TopicosB \
  --username hadoop \
  --base_path /user \
  --source_db curated \
  --num-executors 8 \
  --executor-memory 2g \
  --executor-cores 2 \
  --enable-broadcast  # â† Optimiza JOINs con tablas pequeÃ±as
```
## ğŸ”¹ ğŸ†• Paso 4: Exportar capa Functional a CSV
### âš¡ EXPORTAR CAPA GOLD A CSV
```pyspark
spark-submit /home/hadoop/spark-elt-medallon/procesos/export_gold_to_csv.py
```
Alternativa
```pyspark
spark-submit \
  --master yarn \
  --deploy-mode client \
  --conf spark.sql.warehouse.dir=/user/hadoop/warehouse \
  /home/hadoop/spark-elt-medallon/procesos/export_gold_to_csv.py
```
### ğŸ†• Consolidar archivos part-*.csv en gold.csv
```bash
# Unir todas las particiones en un Ãºnico archivo
cat /user/hadoop/datalake/temp/gold_export/part-*.csv > /user/hadoop/datalake/gold.csv
cp /home/hadoop/spark-elt-medallon/datalake/temp/part-*.csv /home/hadoop/spark-elt-medallon/datalake/gold.csv

# Verificar resultado
wc -l /home/hadoop/spark-elt-medallon/datalake/gold.csv
head -5 /home/hadoop/spark-elt-medallon/datalake/gold.csv
```
## ğŸ”¹ ğŸ†• Paso 5: Migrar gold.csv a MongoDB
### âš¡ EXPORTAR CAPA GOLD.CSV A MONGODB
```pyspark
spark-submit \
  --master yarn \
  --deploy-mode client \
  --packages org.mongodb.spark:mongo-spark-connector_2.12:10.4.0 \
  /home/hadoop/spark-elt-medallon/procesos/export_gold_to_mongo.py
```
### ğŸ”„ Novedades: ExportaciÃ³n CSV + MongoDB
ğŸ†• Â¿QuÃ© se ha integrado recientemente?
- ğŸ“¤ export_gold_to_csv.py
Extrae datos de la capa Functional (Hive) y los exporta a archivo CSV plano
Interoperabilidad con herramientas externas (Excel, Power BI, scripts Python)
- ğŸ” ConsolidaciÃ³n de particiones
Combina mÃºltiples part-*.csv en un Ãºnico gold.csv
Archivo Ãºnico listo para consumo o transferencia

- ğŸ—„ï¸ export_gold_to_mongo.py
Carga el archivo gold.csv a MongoDB usando Spark Connector
Datos listos para APIs, aplicaciones web o microservicios

- ğŸ”— Conector MongoDB oficial
Usa mongo-spark-connector_2.12:10.4.0
ConexiÃ³n segura, tipada y optimizada

### ğŸ“¤ ExportaciÃ³n: Hive â†’ CSV â†’ MongoDB (Detalle TÃ©cnico)
ğŸ” Flujo de transformaciÃ³n de formatos
```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Hive Table     â”‚     â”‚  Archivo CSV    â”‚     â”‚  MongoDB Doc    â”‚
â”‚  (Parquet)      â”‚â”€â”€â”€â”€â–¶â”‚  (Texto plano)  â”‚â”€â”€â”€â”€â–¶â”‚  (BSON/JSON)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“                       â†“                       â†“
â€¢ Columnas tipadas      â€¢ Delimitador: coma       â€¢ Campos como claves
â€¢ Particionado por      â€¢ Encoding: UTF-8         â€¢ Arrays/nested docs
  fecha                 â€¢ Escape de comillas      â€¢ Ãndices configurables
â€¢ Metadatos en Hive     â€¢ Header opcional         â€¢ TTL, sharding, etc.
```
### ğŸ” Consideraciones de seguridad para MongoDB
```bash
# âœ… URI con autenticaciÃ³n (recomendado en producciÃ³n)
mongodb://usuario:password@host:27017/db.collection?authSource=admin&ssl=true

# âœ… Variables de entorno para credenciales (nunca en cÃ³digo)
export MONGO_USER="app_user"
export MONGO_PASS="${MONGO_PASSWORD_SECRET}"
spark-submit ... --conf spark.mongodb.output.uri="mongodb://${MONGO_USER}:${MONGO_PASS}@..."

# âœ… Roles mÃ­nimos en MongoDB
db.grantRolesToUser("app_user", [
  { role: "readWrite", db: "medallon_db" }
])
```
## ğŸ”¹ Paso 6: Detener servicios (opcional)
```bash
stop-yarn.sh
stop-dfs.sh
pkill -f HiveServer2
pkill -f HiveMetaStore
```
# ğŸ“Š Esquema de Datos
Entidad: PERSONA
```table
Campo       Tipo Original       Tipo Final      Regla de Calidad
---------------------------------------------------------------------------
ID          String              String          NOT NULL
NOMBRE      String              String          -
EDAD        String              Integer         BETWEEN 1 AND 99
SALARIO     String              Double          BETWEEN 0.01 AND 9999999.99
ID_EMPRESA  String              String          NOT NULL
```
Entidad: TRANSACCION_ENRIQUECIDA (Functional)
```table
Campo               Origen              TransformaciÃ³n
--------------------------------------------------------------------
ID_PERSONA          TRANSACCION         Clave de join
NOMBRE_PERSONA      PERSONA.NOMBRE      Enriquecimiento semÃ¡ntico
EDAD_PERSONA        PERSONA.EDAD        ConversiÃ³n + validaciÃ³n
TRABAJO_PERSONA     EMPRESA.NOMBRE      JOIN con empresa empleadora
MONTO_TRANSACCION   TRANSACCION.MONTO   ConversiÃ³n a Double
EMPRESA_TRANSACCION EMPRESA.NOMBRE      JOIN con empresa receptora
FECHA_TRANSACCION   TRANSACCION.FECHA   Columna de particiÃ³n
```
# ğŸ’¡ Mejores PrÃ¡cticas Implementadas
- âœ… Esquemas explÃ­citos: Evita inferencia automÃ¡tica y garantiza consistencia
- âœ… ValidaciÃ³n progresiva: Reglas de calidad aplicadas en capa Curated
- âœ… Particionamiento inteligente: Por fecha en tablas transaccionales para consultas eficientes
- âœ… CompresiÃ³n Snappy: Balance Ã³ptimo entre velocidad y almacenamiento 
- âœ… Broadcast joins: OptimizaciÃ³n automÃ¡tica para tablas de dimensiÃ³n pequeÃ±as
- âœ… Logging estructurado: Mensajes claros para monitoreo y debugging
- âœ… ParÃ¡metros configurables: --env, --enable-validation, --enable-broadcast para flexibilidad
- âœ… Limpieza de recursos: spark.stop() y eliminaciÃ³n de vistas temporales  

 Estas prÃ¡cticas siguen recomendaciones de Databricks y Microsoft para pipelines productivos

# ğŸ” Comandos de diagnÃ³stico Ãºtiles
```bash
# Verificar archivos en HDFS
hdfs dfs -ls /user/hadoop/datalake/TOPICOSB_LANDING/

# Consultar metadatos de tabla Hive
hive -e "DESCRIBE FORMATTED topicosb_landing.persona;"
```
```sql
# Contar registros por particiÃ³n
spark.sql("SELECT FECHA_TRANSACCION, COUNT(*) FROM topicosb_functional.transaccion_enriquecida GROUP BY FECHA_TRANSACCION").show()
```
```bash
# Monitorear aplicaciÃ³n Spark en YARN
yarn application -list | grep "Proceso_Carga"
```
# ğŸ¤ Contribuciones
Este proyecto estÃ¡ diseÃ±ado para fines educativos. Â¡Las contribuciones son bienvenidas!

âœ… Ideas para mejorar:
- [ ] Agregar tests unitarios con pytest y chispa
- [ ] Implementar lineage de datos con OpenLineage
- [ ] AÃ±adir dashboard de monitoreo con Prometheus/Grafana
- [ ] Soporte para Delta Lake como formato unificado
- [ ] Docker-compose para entorno de desarrollo local

ğŸ·ï¸ Licencia: MIT - Libre uso para fines educativos y de investigaciÃ³n

ğŸ‘¨â€ğŸ’» Autor: [Jaime Llanos](https://github.com/jllanosb)

ğŸ“… Ãšltima actualizaciÃ³n: Febrero 2026

Contexto: Desarrollado con enfoque en formaciÃ³n en ingenierÃ­a de datos en entornos on-premise

## âœ¨ "La calidad de los datos no es un paso, es un viaje a travÃ©s de capas de refinamiento" âœ¨