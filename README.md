# ğŸ† Spark ETL - Arquitectura MedallÃ³n

Proyecto educativo de ingenierÃ­a de datos que implementa un pipeline ETL con Apache Spark siguiendo el patrÃ³n de arquitectura MedallÃ³n (Bronze â†’ Silver â†’ Gold), adaptado a capas: Workload â†’ Landing â†’ Curated â†’ Functional.

# ğŸ“‹ Tabla de Contenidos
ğŸ¯ Â¿QuÃ© es este proyecto?
ğŸ—ï¸ Arquitectura MedallÃ³n Explicada
ğŸ“ Estructura del Repositorio
âš™ï¸ TecnologÃ­as Utilizadas
ğŸš€ GuÃ­a de EjecuciÃ³n Paso a Paso
ğŸ” Detalle de Cada Capa
ğŸ“Š Esquema de Datos
ğŸ’¡ Mejores PrÃ¡cticas Implementadas
ğŸ”§ SoluciÃ³n de Problemas Comunes
ğŸ“š Recursos de Aprendizaje

# ğŸ¯ Â¿QuÃ© es este proyecto?
Este repositorio es una implementaciÃ³n didÃ¡ctica de un pipeline de datos empresarial usando Apache Spark y Hadoop Ecosystem. Su objetivo principal es:
âœ… EnseÃ±ar los fundamentos de la arquitectura MedallÃ³n en entornos on-premise
âœ… Demonstrar buenas prÃ¡cticas de ingesta, transformaciÃ³n y calidad de datos
âœ… Proveer cÃ³digo reutilizable para procesos ETL escalables
âœ… Facilitar el aprendizaje de Spark SQL, Hive y formatos columnares  

ğŸ’¡ Caso de uso: Procesamiento de transacciones comerciales con entidades PERSONA, EMPRESA y TRANSACCION, aplicando reglas de calidad y enriquecimiento progresivo.

# ğŸ—ï¸ Arquitectura MedallÃ³n Explicada
La arquitectura MedallÃ³n organiza los datos en capas de refinamiento progresivo, mejorando la calidad y utilidad en cada etapa:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FLUJO DE DATOS                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  ğŸ“¥ FUENTES â†’ ğŸ¥‰ WORKLOAD â†’ ğŸ¥ˆ LANDING â†’ ğŸ¥‡ CURATED â†’ âš¡ FUNCTIONAL
â”‚              (Bronze)      (Silver)     (Gold)      (Analytics)
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ”¹ Capa 1: WORKLOAD (Bronze - Datos Crudos)

CaracterÃ­stica      DescripciÃ³n
-----------------------------------------------
Formato             TEXTFILE con delimitador `
Encoding            ISO-8859-1 (soporte legacy)
PropÃ³sito           Ingesta fiel de fuentes originales
ValidaciÃ³n          MÃ­nima (solo estructura)

ğŸ”¹ Capa 2: LANDING (Silver - Datos Estandarizados)

CaracterÃ­stica      DescripciÃ³n
-----------------------------------------------
Formato             AVRO con compresiÃ³n Snappy 
Schema              Definido en archivos .avsc
PropÃ³sito           Estructura consistente + metadatos
Particionamiento    Por fecha en tablas transaccionales

ğŸ”¹ Capa 3: CURATED (Gold - Datos Limpios)

CaracterÃ­stica      DescripciÃ³n
-----------------------------------------
Formato             Parquet con Snappy
Calidad             Reglas de validaciÃ³n aplicadas
Tipado              ConversiÃ³n explÃ­cita de tipos
PropÃ³sito           Datos confiables para anÃ¡lisis

ğŸ”¹ Capa 4: FUNCTIONAL (Analytics - Datos Enriquecidos)

CaracterÃ­stica      DescripciÃ³n
Formato             Parquet optimizado
TransformaciÃ³n      JOINs para enriquecimiento semÃ¡ntico
OptimizaciÃ³n        Broadcast joins para tablas pequeÃ±as
PropÃ³sito           Listo para dashboards y ML

ğŸ“š La arquitectura MedalliÃ³n es ampliamente adoptada en plataformas como Databricks y Azure Synapse para organizar data lakes de forma escalable.

# ğŸ“ Estructura del Repositorio

spark-elt-medallon/
â”‚
â”œâ”€â”€ ğŸ“ dataset/                    # Datos fuente de ejemplo
â”‚   â”œâ”€â”€ empresa.data              # CatÃ¡logo de empresas (pipe-delimited)
â”‚   â”œâ”€â”€ persona.data              # Registro de personas
â”‚   â””â”€â”€ transacciones.data        # Movimientos comerciales
â”‚
â”œâ”€â”€ ğŸ“ schema/                     # Esquemas Avro para validaciÃ³n
â”‚   â”œâ”€â”€ empresa.avsc              # Schema: id, nombre
â”‚   â”œâ”€â”€ persona.avsc              # Schema: id, nombre, contacto, etc.
â”‚   â””â”€â”€ transaccion.avsc          # Schema: monto, fecha, relaciones
â”‚
â”œâ”€â”€ ğŸ“ procesos/                   # Scripts PySpark del pipeline
â”‚   â”œâ”€â”€ poblar_capa_workload.py   # â–¶ï¸ Ingesta inicial (CSV â†’ Hive TEXTFILE)
â”‚   â”œâ”€â”€ poblar_capa_landing.py    # â–¶ï¸ EstandarizaciÃ³n (â†’ Avro + particiÃ³n)
â”‚   â”œâ”€â”€ poblar_capa_curated.py    # â–¶ï¸ Limpieza y validaciÃ³n de calidad
â”‚   â””â”€â”€ poblar_capa_functional.py # â–¶ï¸ Enriquecimiento con JOINs
â”‚
â”œâ”€â”€ ğŸ“„ instrucciones.txt          # GuÃ­a rÃ¡pida de comandos de ejecuciÃ³n
â””â”€â”€ ğŸ“„ README.md                  # Â¡Este archivo! DocumentaciÃ³n didÃ¡ctica

# âš™ï¸ TecnologÃ­as Utilizadas

TecnologÃ­a          VersiÃ³n         PropÃ³sito
------------------------------------------------------------------------
Apache Spark        3.5.0           Motor de procesamiento distribuido
Apache Hive         3.x             Metastore y consulta SQL sobre HDFS
Hadoop HDFS         3.x             Almacenamiento distribuido
Apache YARN         3.x             Gestor de recursos del cluster
Formato Avro        1.11+           SerializaciÃ³n con esquema evolutivo
Formato Parquet     1.12+           Almacenamiento columnar optimizado
CompresiÃ³n Snappy   1.1+            Balance velocidad/tamaÃ±o en datos

ğŸ”— Estas herramientas son estÃ¡ndar en ecosistemas de Big Data on-premise y en la nube

# ğŸš€ GuÃ­a de EjecuciÃ³n Paso a Paso




    ğŸ·ï¸ Licencia: MIT - Libre uso para fines educativos y de investigaciÃ³n
    ğŸ‘¨â€ğŸ’» Autor: @jllanosb

    ğŸ“… Ãšltima actualizaciÃ³n: Febrero 2026
    ğŸ‡µğŸ‡ª Contexto: Desarrollado con enfoque en formaciÃ³n en ingenierÃ­a de datos en entornos on-premise

âœ¨ "La calidad de los datos no es un paso, es un viaje a travÃ©s de capas de refinamiento" âœ¨